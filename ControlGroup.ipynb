{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mydataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'mode': {'load_mode': False}, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 256, 'num_layers': 2, 'bidirectional': True, 'history_step_size': 1, 'history_num_frames': 49, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_mode': 0, 'raster_size': [112, 112], 'pixel_size': [0.75, 0.75], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 2}, 'val_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 2}, 'train_params': {'device': 1, 'epochs': 1}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ===== INIT DATASET\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset)\n",
    "    print(train_dataset[0].keys())\n",
    "\n",
    "    train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"], \n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        prefetch_factor = 2,\n",
    "        pin_memory = True,\n",
    "        persistent_workers=True,\n",
    "        worker_init_fn=my_dataset_worker_init_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "num_classes = 3 # 类数\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional,batch_first=True)\n",
    "        self.encoder2 = nn.Linear(latent_dim*(1+bidirectional), num_decoder_tokens)\n",
    "\n",
    "    def forward(self, data):\n",
    "        inputs1 = torch.FloatTensor(data[\"history_positions\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1 = torch.unsqueeze(inputs1,0)\n",
    "\n",
    "        h0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "\n",
    "        out1, _ = self.encoder(inputs1, (h0, c0))\n",
    "        y_hat = torch.tanh(self.encoder2(out1))\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "def loss_function(y_hat, data):\n",
    "    y_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "    MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "    MSE = MSE * y_availabilities\n",
    "    MSE = MSE.mean()\n",
    "    return MSE\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6978 [00:00<?, ?it/s]D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "loss: 0.04639032110571861 loss(avg): 23.47759186104287: 100%|██████████████████████| 6978/6978 [10:51<00:00, 10.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "            y_hat = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = loss_function(y_hat, data)\n",
    "            else:\n",
    "                loss = loss_function(y_hat, data)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward() \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                cvae_optimizer.step()\n",
    "                cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "    #         cvae_optimizer.zero_grad(set_to_none = True)\n",
    "    #         loss.backward()\n",
    "    #         cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    torch.save(cvae.state_dict(),'E:/Downloads/cvae.pth')\n",
    "    plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode'] and cfg['train_params']['epochs'] > 1: \n",
    "    plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n",
      "6978\n"
     ]
    }
   ],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset[0].keys())\n",
    "# print(len(eval_dataset))\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    prefetch_factor = 2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory = True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")\n",
    "\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/6978 [00:00<?, ?it/s]D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6978/6978 [10:40<00:00, 10.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.load_state_dict(torch.load('E:/Downloads/cvae.pth'))\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "confs = []\n",
    "tr_it = iter(eval_dataloader)\n",
    "progress_bar = tqdm(range(len(eval_dataloader)),position=0)\n",
    "\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(eval_dataloader)\n",
    "        data = next(tr_it)\n",
    "    y_hat = cvae(data)\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.detach().cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_offset = transform_points(agents_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "\n",
    "    \n",
    "pred_path = \"E:/Downloads/pred.csv\"\n",
    "eval_gt_path = \"E:/Downloads/gt.csv\"\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[9], FDE[29], FDE[49], np.mean(FDE[:10]), np.mean(FDE[:30]), np.mean(FDE[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vis = False\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    predicted_positions1 = []\n",
    "    predicted_positions2 = []\n",
    "    predicted_positions3 = []\n",
    "    target_positions = []\n",
    "\n",
    "    if multi_vis == True:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "\n",
    "            out_net1 = out_net[0][0]\n",
    "            out_net2 = out_net[0][1]\n",
    "            out_net3 = out_net[0][2]\n",
    "            out_pos1 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos2 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos3 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions1.append(transform_points(out_pos1, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions2.append(transform_points(out_pos2, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions3.append(transform_points(out_pos3, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions1 = transform_points(np.concatenate(predicted_positions1), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions2 = transform_points(np.concatenate(predicted_positions2), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions3 = transform_points(np.concatenate(predicted_positions3), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, predicted_positions1, (34,222,79))\n",
    "        draw_trajectory(im_ego, predicted_positions2, (220,235,21))\n",
    "        draw_trajectory(im_ego, predicted_positions3, PREDICTED_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "    #         print(confs)\n",
    "            out_net = out_net[0][np.argmax(confs)]\n",
    "            out_pos = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python {lgsvl}",
   "language": "python",
   "name": "lgsvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
