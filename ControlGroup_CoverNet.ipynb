{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:17.456143Z",
     "start_time": "2022-01-14T03:53:15.939567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\stgm\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as f\n",
    "\n",
    "from newdataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:17.472158Z",
     "start_time": "2022-01-14T03:53:17.457144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'mode': {'load_mode': False}, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 256, 'num_layers': 2, 'bidirectional': True, 'history_step_size': 1, 'history_num_frames': 9, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True, 'num_classes': 10}, 'raster_params': {'raster_mode': 1, 'raster_size': [100, 100], 'pixel_size': [0.2, 0.2], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 6}, 'val_data_loader': {'key': 'scenes/validate.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 4}, 'scale': 1, 'train_params': {'device': 1, 'epochs': 2}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:24.839436Z",
     "start_time": "2022-01-14T03:53:17.473159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234443\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ===== INIT DATASET\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset)\n",
    "    print(train_dataset[0].keys())\n",
    "\n",
    "    train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"], num_classes=cfg[\"model_params\"][\"num_classes\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"], \n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        prefetch_factor = 2,\n",
    "        pin_memory = True,\n",
    "        persistent_workers=True,\n",
    "        worker_init_fn=my_dataset_worker_init_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:24.887482Z",
     "start_time": "2022-01-14T03:53:24.841438Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数\n",
    "\n",
    "num_classes = cfg[\"model_params\"][\"num_classes\"] # 类数\n",
    "modal_fc = latent_dim*(1+bidirectional) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:24.902493Z",
     "start_time": "2022-01-14T03:53:24.889483Z"
    }
   },
   "outputs": [],
   "source": [
    "def neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    del gt, avails, max_value\n",
    "    return torch.mean(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T03:53:27.427214Z",
     "start_time": "2022-01-14T03:53:24.903494Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Callable, Union\n",
    "from nuscenes.prediction.models.backbone import calculate_backbone_feature_dim\n",
    "\n",
    "# Number of entries in Agent State Vector\n",
    "ASV_DIM = 3\n",
    "\n",
    "\n",
    "class CoverNet(nn.Module):\n",
    "    \"\"\" Implementation of CoverNet https://arxiv.org/pdf/1911.10298.pdf \"\"\"\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, num_modes: int,\n",
    "                 n_hidden_layers: List[int] = None,\n",
    "                 input_shape: Tuple[int, int, int] = (3, 500, 500)):\n",
    "        \"\"\"\n",
    "        Inits Covernet.\n",
    "        :param backbone: Backbone model. Typically ResNetBackBone or MobileNetBackbone\n",
    "        :param num_modes: Number of modes in the lattice\n",
    "        :param n_hidden_layers: List of dimensions in the fully connected layers after the backbones.\n",
    "            If None, set to [4096]\n",
    "        :param input_shape: Shape of image input. Used to determine the dimensionality of the feature\n",
    "            vector after the CNN backbone.\n",
    "        \"\"\"\n",
    "\n",
    "        if n_hidden_layers and not isinstance(n_hidden_layers, list):\n",
    "            raise ValueError(f\"Param n_hidden_layers must be a list. Received {type(n_hidden_layers)}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not n_hidden_layers:\n",
    "            n_hidden_layers = [4096]\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        backbone_feature_dim = calculate_backbone_feature_dim(backbone, input_shape)\n",
    "        n_hidden_layers = [backbone_feature_dim + ASV_DIM] + n_hidden_layers + [num_modes]\n",
    "\n",
    "        linear_layers = [nn.Linear(in_dim, out_dim)\n",
    "                         for in_dim, out_dim in zip(n_hidden_layers[:-1], n_hidden_layers[1:])]\n",
    "\n",
    "        self.head = nn.ModuleList(linear_layers)\n",
    "\n",
    "    def forward(self, image_tensor: torch.Tensor,\n",
    "                agent_state_vector: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        :param image_tensor: Tensor of images in the batch.\n",
    "        :param agent_state_vector: Tensor of agent state vectors in the batch\n",
    "        :return: Logits for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        backbone_features = self.backbone(image_tensor)\n",
    "\n",
    "        logits = torch.cat([backbone_features, agent_state_vector], dim=1)\n",
    "\n",
    "        for linear in self.head:\n",
    "            logits = linear(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def mean_pointwise_l2_distance(lattice: torch.Tensor, ground_truth: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the index of the closest trajectory in the lattice as measured by l1 distance.\n",
    "    :param lattice: Lattice of pre-generated trajectories. Shape [num_modes, n_timesteps, state_dim]\n",
    "    :param ground_truth: Ground truth trajectory of agent. Shape [1, n_timesteps, state_dim].\n",
    "    :return: Index of closest mode in the lattice.\n",
    "    \"\"\"\n",
    "    stacked_ground_truth = ground_truth.repeat(lattice.shape[0], 1, 1)\n",
    "    return torch.pow(lattice - stacked_ground_truth, 2).sum(dim=2).sqrt().mean(dim=1).argmin()\n",
    "\n",
    "\n",
    "class ConstantLatticeLoss:\n",
    "    \"\"\"\n",
    "    Computes the loss for a constant lattice CoverNet model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lattice: Union[np.ndarray, torch.Tensor],\n",
    "                 similarity_function: Callable[[torch.Tensor, torch.Tensor], int] = mean_pointwise_l2_distance):\n",
    "        \"\"\"\n",
    "        Inits the loss.\n",
    "        :param lattice: numpy array of shape [n_modes, n_timesteps, state_dim]\n",
    "        :param similarity_function: Function that computes the index of the closest trajectory in the lattice\n",
    "            to the actual ground truth trajectory of the agent.\n",
    "        \"\"\"\n",
    "\n",
    "        self.lattice = torch.Tensor(lattice)\n",
    "        self.similarity_func = similarity_function\n",
    "\n",
    "    def __call__(self, batch_logits: torch.Tensor, batch_ground_truth_trajectory: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss on a batch.\n",
    "        :param batch_logits: Tensor of shape [batch_size, n_modes]. Output of a linear layer since this class\n",
    "            uses nn.functional.cross_entropy.\n",
    "        :param batch_ground_truth_trajectory: Tensor of shape [batch_size, 1, n_timesteps, state_dim]\n",
    "        :return: Average element-wise loss on the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # If using GPU, need to copy the lattice to the GPU if haven't done so already\n",
    "        # This ensures we only copy it once\n",
    "        if self.lattice.device != batch_logits.device:\n",
    "            self.lattice = self.lattice.to(batch_logits.device)\n",
    "\n",
    "        batch_losses = torch.Tensor().requires_grad_(True).to(batch_logits.device)\n",
    "\n",
    "        for logit, ground_truth in zip(batch_logits, batch_ground_truth_trajectory):\n",
    "\n",
    "            closest_lattice_trajectory = self.similarity_func(self.lattice, ground_truth)\n",
    "            label = torch.LongTensor([closest_lattice_trajectory]).to(batch_logits.device)\n",
    "            classification_loss = f.cross_entropy(logit.unsqueeze(0), label)\n",
    "\n",
    "            batch_losses = torch.cat((batch_losses, classification_loss.unsqueeze(0)), 0)\n",
    "\n",
    "        return batch_losses.mean()\n",
    "\n",
    "class CoverNet_train:\n",
    "    def __init__(self, config_file, verbose):\n",
    "        self.parser = Json_Parser(config_file)\n",
    "        self.config = self.parser.load_parser()    \n",
    "        self.device = torch.device(self.config['LEARNING']['device'] if torch.cuda.is_available() else 'cpu')\n",
    "        self.lr = self.config['LEARNING']['lr']\n",
    "        self.momentum = self.config['LEARNING']['momentum']\n",
    "        self.n_epochs = self.config['LEARNING']['n_epochs']\n",
    "        self.batch_size = self.config['LEARNING']['batch_size']\n",
    "        self.val_batch_size = self.config['LEARNING']['val_batch_size']\n",
    "        self.num_val_data = self.config['LEARNING']['num_val_data']\n",
    "        self.num_modes = self.config['LEARNING']['num_modes']\n",
    "        self.print_size = self.config['LEARNING']['print_size']\n",
    "\n",
    "        self.train_dataset = DataLoader(NuSceneDataset_CoverNet(train_mode=True, config_file_name=config_file, verbose=verbose), batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        self.val_dataset = DataLoader(NuSceneDataset_CoverNet(train_mode=False, config_file_name=config_file, verbose=verbose), batch_size=self.val_batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "        self.backbone = ResNetBackbone('resnet50')\n",
    "        # self.resnet_path = self.config['LEARNING']['weight_path']\n",
    "        # self.backbone.load_state_dict(torch.load(self.resnet_path))\n",
    "        self.model = CoverNet(self.backbone, self.num_modes)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum) \n",
    "        \n",
    "        ###############################################################\n",
    "        # self.criterion = nn.CrossEntropyLoss()           ## classification loss\n",
    "        self.traj_set_path = self.config['LEARNING']['trajectory_set_path']\n",
    "        self.trajectories_set =torch.Tensor(pickle.load(open(self.traj_set_path, 'rb')))\n",
    "        self.criterion = ConstantLatticeLoss(self.trajectories_set)\n",
    "        ###############################################################\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.save_name = datetime.now().strftime(\"%Y%m%d-%H_%M_%S\")\n",
    "        self.writer = SummaryWriter('./result/tensorboard/' + self.save_name)\n",
    "        self.net_save_path = os.path.join(self.config['LEARNING']['model_save_path'], self.save_name)\n",
    "        if not os.path.exists(self.net_save_path):\n",
    "            os.mkdir(self.net_save_path)\n",
    "        self.writer.add_text('Config', json.dumps(self.config))\n",
    "\n",
    "        dataset_info = {'train_size' : self.train_dataset.__len__(), 'val_size' : self.val_dataset.__len__(), \n",
    "                        'train_batch_size' : self.batch_size, 'val_batch_size' : self.val_batch_size}\n",
    "        self.writer.add_text('Dataset_size', json.dumps(dataset_info))\n",
    "\n",
    "    def get_label(self, traj, future):\n",
    "        scores = torch.full((len(traj),),1e4)\n",
    "        for i in range(len(traj)):\n",
    "            if (torch.norm(traj[i,-1]-future[-1]) < 10):    \n",
    "                scores[i]= torch.norm(traj[i]-future)\n",
    "            \n",
    "        ind=torch.argmin(scores)\n",
    "        \n",
    "        res=torch.zeros_like(scores)\n",
    "        res[ind] =1\n",
    "\n",
    "        return res, ind\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print(\"CoverNet learning starts!\")\n",
    "        step = 1\n",
    "        best_val_loss = 10000\n",
    "        for epoch in range(self.n_epochs + 1):\n",
    "            Loss, Val_Loss = [], []\n",
    "\n",
    "            for data in self.train_dataset:\n",
    "                # train_mode\n",
    "                self.model.train()\n",
    "\n",
    "                img_tensor = data['img'].to(device=self.device)\n",
    "                agent_state_tensor = torch.Tensor(data['ego_state'].tolist()).to(self.device)\n",
    "                agent_state_tensor = torch.squeeze(agent_state_tensor, 1)\n",
    "\n",
    "                prediction = self.model(img_tensor, agent_state_tensor)\n",
    "                # label = data['label']\n",
    "                label, anchor_ind = self.get_label(self.trajectories_set, data['future_local_ego_pos'])\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.criterion(prediction,label)\n",
    "\n",
    "                ## for calculating gt loss\n",
    "                # label_onehot = F.one_hot(label, num_classes=self.num_modes)\n",
    "                # gt_loss = self.criterion(label_onehot.float(),label)\n",
    "                # print(\"gt_loss : \",gt_loss)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                step += 1\n",
    "                with torch.no_grad():\n",
    "                    Loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "                if step % self.print_size == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # eval_mode\n",
    "                        self.model.eval()\n",
    "\n",
    "                        k = 0\n",
    "                        for val_data in self.val_dataset:\n",
    "                            img_tensor = val_data['img'].to(self.device)\n",
    "                            agent_state_tensor = torch.Tensor(val_data['ego_state'].tolist()).to(self.device)\n",
    "                            agent_state_tensor = torch.squeeze(agent_state_tensor, 1)\n",
    "\n",
    "                            prediction = self.model(img_tensor, agent_state_tensor)\n",
    "                            label = val_data['label']\n",
    "\n",
    "                            val_loss = self.criterion(prediction,label)\n",
    "                            Val_Loss.append(val_loss.detach().cpu().numpy())\n",
    "\n",
    "                            pred = F.softmax(prediction,dim=-1)\n",
    "                            self.writer.add_figure('Results', self.plot_results(val_data,pred,anchor_ind), step)\n",
    "                            k += 1\n",
    "                            if(k == self.num_val_data):\n",
    "                                break\n",
    "                            \n",
    "                    loss = np.array(Loss).mean()\n",
    "                    val_loss = np.array(Val_Loss).mean()\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        save_path = os.path.join(self.net_save_path, 'best_val_loss_model.pth')\n",
    "                        torch.save(self.model.state_dict(), save_path)\n",
    "                        self.writer.add_scalar('Best Val Loss', best_val_loss)\n",
    "\n",
    "                    self.writer.add_scalar('Loss', loss, step)\n",
    "                    self.writer.add_scalar('Val Loss', val_loss, step)\n",
    "\n",
    "\n",
    "                    print(\"Epoch: {}/{} | Step: {} | Loss: {:.5f} | Val_Loss: {:.5f}\".format(\n",
    "                            epoch + 1, self.n_epochs, step, loss, val_loss))                    \n",
    "                    Loss, Val_Loss = [], []\n",
    "        \n",
    "            save_path = os.path.join(self.net_save_path, 'epoch_{0}.pth'.format(epoch + 1))\n",
    "            torch.save(self.model.state_dict(), save_path)\n",
    "\n",
    "        save_path = os.path.join(self.net_save_path, 'CoverNet.pth')\n",
    "        torch.save(self.model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.950Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 111.06519024353474 loss(avg): 703.0764715437928:  11%|██▍                   | 1656/14653 [06:23<47:05,  4.60it/s]"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)//cfg['scale']),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data,label = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data,label = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "            y_hat, confidences, mean1, std1, mean2, std2 = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    NLL,KLD,Cross = loss_function(y_hat, confidences, data, label, mean1, std1, mean2, std2)\n",
    "                    loss = NLL + (25)*KLD + 20*Cross\n",
    "#                     if i + 1>= len(train_dataloader)//1:\n",
    "#                         print(NLL,KLD,Cross)\n",
    "            else:\n",
    "                NLL,KLD,Cross = loss_function(y_hat, confidences, data, label, mean1, std1, mean2, std2)\n",
    "                loss = NLL + (25)*KLD + 20*Cross\n",
    "#                 if i + 1>= len(train_dataloader)//1:\n",
    "#                     print(NLL,KLD,Cross)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "#             loss = loss / accumulation_steps\n",
    "#             loss.backward() \n",
    "#             if (i+1) % accumulation_steps == 0:\n",
    "#                 cvae_optimizer.step()\n",
    "#                 cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "            cvae_optimizer.zero_grad(set_to_none = True)\n",
    "            loss.backward()\n",
    "            cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "            del data, y_hat, confidences, mean1, std1, mean2, std2, NLL, KLD, Cross, loss\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covernet_train = CoverNet_train(config_file='./covernet_config.json', verbose=False)\n",
    "covernet_train.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.951Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    torch.save(cvae.state_dict(),'E:/Downloads/lyft-motion-prediction-autonomous-vehicles/cvae.pth')\n",
    "    plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.953Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode'] and cfg['train_params']['epochs'] > 1: \n",
    "    plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.954Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset[0].keys())\n",
    "# print(len(eval_dataset))\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    prefetch_factor = 2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory = True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")\n",
    "pred_path = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles/pred.csv\"\n",
    "eval_gt_path = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles/gt.csv\"\n",
    "cvae.load_state_dict(torch.load('E:/Downloads/lyft-motion-prediction-autonomous-vehicles/cvae.pth'))\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "losses_test = []\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "confs = []\n",
    "tr_it = iter(eval_dataloader)\n",
    "progress_bar = tqdm(range(len(eval_dataloader)//cfg['scale']),position=0)\n",
    "\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data,_ = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(eval_dataloader)\n",
    "        data,_ = next(tr_it)\n",
    "    y_hat, confidences,mean1,std1,mean2,std2 = cvae(data)\n",
    "#     if cfg[\"train_params\"][\"device\"] == 1:\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             NLL,KLD,Cross = loss_function(y_hat, confidences, data, mean1, std1, mean2, std2)\n",
    "#             loss = NLL + (25)*KLD + 20*Cross\n",
    "#     losses_test.append(loss.item())\n",
    "#     progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_test)}\")\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.detach().cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_off = []\n",
    "    for i in range(num_classes):\n",
    "        coords_off.append(transform_points(agents_coords[:,i,:,:], world_from_agents) - centroids[:, None, :2])\n",
    "#         coords_offset2 = transform_points(agents_coords[:,1,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "#         coords_offset3 = transform_points(agents_coords[:,2,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "    coords_offset = np.stack([coords_offseti for coords_offseti in coords_off],1)\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "    confs.append(confidences.detach().cpu().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.957Z"
    }
   },
   "outputs": [],
   "source": [
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "               confs=np.concatenate(confs)\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.959Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[10//cfg[\"model_params\"][\"future_step_size\"]-1], FDE[30//cfg[\"model_params\"][\"future_step_size\"]-1], FDE[50//cfg[\"model_params\"][\"future_step_size\"]-1], np.mean(FDE[:10//cfg[\"model_params\"][\"future_step_size\"]]), np.mean(FDE[:30//cfg[\"model_params\"][\"future_step_size\"]]), np.mean(FDE[:50//cfg[\"model_params\"][\"future_step_size\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T03:53:15.960Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_vis = False\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    predicted_positions1 = []\n",
    "    predicted_positions2 = []\n",
    "    predicted_positions3 = []\n",
    "    target_positions = []\n",
    "\n",
    "    if multi_vis == True:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "            print(confs)\n",
    "            out_net1 = out_net[0][0]\n",
    "            out_net2 = out_net[0][1]\n",
    "            out_net3 = out_net[0][2]\n",
    "            out_pos1 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos2 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos3 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions1.append(transform_points(out_pos1, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions2.append(transform_points(out_pos2, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions3.append(transform_points(out_pos3, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions1 = transform_points(np.concatenate(predicted_positions1), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions2 = transform_points(np.concatenate(predicted_positions2), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions3 = transform_points(np.concatenate(predicted_positions3), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, predicted_positions1, (34,222,79))\n",
    "        draw_trajectory(im_ego, predicted_positions2, (220,235,21))\n",
    "        draw_trajectory(im_ego, predicted_positions3, PREDICTED_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "    #         print(confs)\n",
    "            out_net = out_net[0][np.argmax(confs)]\n",
    "            out_pos = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python {lgsvl}",
   "language": "python",
   "name": "lgsvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
