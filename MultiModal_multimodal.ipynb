{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:35.104969Z",
     "start_time": "2022-01-15T07:24:33.635463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\stgm\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from newdataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:35.120984Z",
     "start_time": "2022-01-15T07:24:35.105971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'mode': {'load_mode': False}, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 256, 'num_layers': 2, 'bidirectional': True, 'history_step_size': 1, 'history_num_frames': 9, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True, 'num_classes': 10}, 'raster_params': {'raster_mode': 1, 'raster_size': [240, 240], 'pixel_size': [0.2, 0.2], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 6}, 'val_data_loader': {'key': 'scenes/validate.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 4}, 'scale': 1, 'train_params': {'device': 1, 'epochs': 2}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:42.027592Z",
     "start_time": "2022-01-15T07:24:35.121985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234443\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ===== INIT DATASET\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset)\n",
    "    print(train_dataset[0].keys())\n",
    "\n",
    "    train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"], num_classes=cfg[\"model_params\"][\"num_classes\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"], \n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        prefetch_factor = 2,\n",
    "        pin_memory = True,\n",
    "        persistent_workers=True,\n",
    "        worker_init_fn=my_dataset_worker_init_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:42.075137Z",
     "start_time": "2022-01-15T07:24:42.028594Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数\n",
    "\n",
    "num_classes = cfg[\"model_params\"][\"num_classes\"] # 类数\n",
    "modal_fc = latent_dim*(1+bidirectional) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:42.091152Z",
     "start_time": "2022-01-15T07:24:42.076139Z"
    }
   },
   "outputs": [],
   "source": [
    "def neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    del gt, avails, max_value\n",
    "    return torch.mean(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-15T07:24:44.493351Z",
     "start_time": "2022-01-15T07:24:42.092154Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # 定义序列编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.encoder2 = nn.Linear(latent_dim*(1+bidirectional), encoder_fc)\n",
    "#         self.encoder2 = nn.Linear(latent_dim*(1+bidirectional)+modal_fc, encoder_fc)\n",
    "#         self.encoder_mean1 = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.encoder_mean2 = nn.Linear(encoder_fc, z_dimension)\n",
    "#         self.encoder_std1 = nn.Linear(latent_dim*(1+bidirectional), 32)\n",
    "        self.encoder_std2 = nn.Linear(encoder_fc, z_dimension)\n",
    "\n",
    "        # 定义序列解码器\n",
    "        self.decoder = nn.LSTM(z_dimension*2, latent_dim, num_layers=num_layers,\n",
    "                               bidirectional=bidirectional, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.decoder_fc1 = nn.Linear(64, num_decoder_tokens*num_classes)\n",
    "#         self.decoder_fc2 = nn.Linear(64, num_decoder_tokens)\n",
    "#         self.decoder_fc3 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_confi = nn.Linear(num_decoder_tokens*num_classes, num_classes)\n",
    "\n",
    "        # 定义图像编码器\n",
    "        # load pre-trained Conv2D model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        # change input channels number to match the rasterizer's output\n",
    "        num_history_channels = (\n",
    "            cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.resnet.conv1.out_channels,\n",
    "            kernel_size=self.resnet.conv1.kernel_size,\n",
    "            stride=self.resnet.conv1.stride,\n",
    "            padding=self.resnet.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # change output size to (X, Y) * number of future states\n",
    "        num_targets = z_dimension * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=512)\n",
    "        self.encoder_mean3 = nn.Linear(512, num_targets)\n",
    "        self.encoder_std3 = nn.Linear(512, num_targets)\n",
    "        \n",
    "        #定义采样器\n",
    "#         self.sampler_fc1 = nn.Linear(3,1024)\n",
    "#         self.sampler_fc2 = nn.Linear(1024,512)\n",
    "#         self.sampler_fc3 = nn.Linear(512,z_dimension * cfg[\"model_params\"][\"future_num_frames\"])\n",
    "        \n",
    "        #定义行为预测\n",
    "        self.modal1 = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.modal2 = nn.Linear(latent_dim*(1+bidirectional), num_classes)\n",
    "        \n",
    "    def noise_reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        del eps\n",
    "        return z\n",
    "\n",
    "    def forward(self, data):\n",
    "        inputs1 = torch.FloatTensor(data[\"history_positions\"]).to(device)\n",
    "#         yaw = torch.FloatTensor(data[\"history_yaws\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1 = torch.unsqueeze(inputs1, 0)\n",
    "\n",
    "        h0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "\n",
    "        inputs2 = torch.FloatTensor(data[\"image\"]).to(device)\n",
    "        if inputs2.dim() == 3:\n",
    "            inputs2 = torch.unsqueeze(inputs2, 0)\n",
    "\n",
    "        out1, _ = self.encoder(inputs1, (h0, c0))\n",
    "#         out1 = out1[:,-1,:]\n",
    "#         out1 = torch.unsqueeze(out1, 1)\n",
    "#         out1 = out1.expand(out1.size()[0],decoder_length,out1.size()[-1])\n",
    "        out1 = F.relu(self.encoder2(out1), inplace=True)\n",
    "        \n",
    "        out_modal, _ = self.modal1(inputs1, (h0, c0))\n",
    "        out_modal = F.softmax(self.modal2(out_modal[:, -1, :]), dim = -1)\n",
    "\n",
    "#         mean1 = F.relu(self.encoder_mean1(out1), inplace=True)\n",
    "        mean2 = F.relu(self.encoder_mean2(out1), inplace=True)\n",
    "#         logstd1 = F.relu(self.encoder_std1(out1), inplace=True)\n",
    "        logstd2 = F.relu(self.encoder_std2(out1), inplace=True)\n",
    "        # prevent from poster vanish\n",
    "#         logstd2 = torch.abs(logstd2) + 0.6\n",
    "\n",
    "        z1 = self.noise_reparameterize(mean2, logstd2)\n",
    "        z1 = z1[:, -1, :]\n",
    "        z1 = torch.unsqueeze(z1, 1)\n",
    "        z1 = z1.expand(z1.size()[0], decoder_length, z1.size()[-1])\n",
    "\n",
    "        out12 = self.resnet(inputs2)\n",
    "        mean3 = F.relu(self.encoder_mean3(out12), inplace=True)\n",
    "        logstd3 = F.relu(self.encoder_std3(out12), inplace=True)\n",
    "        z2 = self.noise_reparameterize(mean3, logstd3)\n",
    "        z2 = z2.reshape(z1.size())\n",
    "        z = torch.cat([z1, z2], -1)\n",
    "        out2, _ = self.decoder(z)\n",
    "        out2 = F.relu(self.decoder_fc(out2), inplace=True)\n",
    "\n",
    "        out21 = F.relu(self.decoder_fc1(out2), inplace=True)\n",
    "#         out22 = F.relu(self.decoder_fc2(out2), inplace=True)\n",
    "#         out23 = F.relu(self.decoder_fc3(out2), inplace=True)\n",
    "        confidences = F.softmax(self.decoder_confi(out21)[:, -1, :], dim=-1)\n",
    "        confidences = F.softmax(confidences * out_modal, dim=-1)\n",
    "        \n",
    "        out3 = torch.split(out21,2,dim=-1)\n",
    "        y_hat = torch.Tensor([]).to(device)\n",
    "        for i in out3:\n",
    "            i = torch.unsqueeze(i, 1) \n",
    "            y_hat=torch.cat([y_hat,i],dim=1)\n",
    "\n",
    "        return y_hat, confidences, mean2, logstd2, mean3, logstd3\n",
    "\n",
    "\n",
    "# def label_maker(yaw,target_yaw,num_classes):\n",
    "# #     target_yaw = target_yaw.squeeze()\n",
    "# #     print(yaw.size())\n",
    "#     target_yaw = torch.max(target_yaw, dim=1)\n",
    "#     target_yaw = target_yaw.squeeze(dim=1)\n",
    "#     label = torch.zeros_like(yaw)\n",
    "#     phi = 2*np.pi / num_classes\n",
    "#     diff = target_yaw - yaw\n",
    "#     for k in range(num_classes):\n",
    "#         if np.pi-(k+1)*phi<diff<np.pi-k*phi:\n",
    "#             label[k]=1\n",
    "#     del phi, diff\n",
    "#     del yaw, ind, one, label1, label2, label3\n",
    "#     return w\n",
    "\n",
    "\n",
    "def loss_function(y_hat, confidences, data, label, mean1, std1, mean2, std2):\n",
    "    y_availabilities = data[\"target_availabilities\"].to(device)\n",
    "#     yaw = data[\"target_yaws\"].to(device)\n",
    "#     label = label_maker(yaw)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "#     MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "#     MSE = MSE * y_availabilities\n",
    "#     MSE = MSE.mean()\n",
    "    label = label.to(device)\n",
    "    Cross = F.binary_cross_entropy_with_logits(confidences, label)\n",
    "    NLL = neg_multi_log_likelihood_batch(\n",
    "        y_true, y_hat, confidences, y_availabilities)\n",
    "    # 因为var是标准差的自然对数，先求自然对数然后平方转换成方差\n",
    "    var1 = torch.pow(torch.exp(std1), 2)\n",
    "    var2 = torch.pow(torch.exp(std2), 2)\n",
    "    KLD1 = -0.5 * torch.mean(1+torch.log(var1)-torch.pow(mean1, 2)-var1)\n",
    "    KLD1 = torch.max(KLD1,torch.ones_like(KLD1))\n",
    "    KLD2 = -0.5 * torch.mean(1+torch.log(var2)-torch.pow(mean2, 2)-var2)\n",
    "    KLD2 = torch.max(KLD2,torch.ones_like(KLD2))\n",
    "    KLD = KLD1 + KLD2\n",
    "#     print('KLD: ',KLD,' NLL: ',NLL,' Cross: ', Cross)\n",
    "    del KLD1, KLD2, var1, var2, y_availabilities, y_true\n",
    "    return NLL, KLD, Cross\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "# vae.load_state_dict(torch.load('./VAE_z2.pth'))\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 82.42072959896177 loss(avg): 217.97686636191582:  77%|██████████████▌    | 11245/14653 [1:11:43<11:08,  5.10it/s]"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)//cfg['scale']),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data,label = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data,label = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "            y_hat, confidences, mean1, std1, mean2, std2 = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    NLL,KLD,Cross = loss_function(y_hat, confidences, data, label, mean1, std1, mean2, std2)\n",
    "                    loss = NLL + (25)*KLD + 20*Cross\n",
    "#                     if i + 1>= len(train_dataloader)//1:\n",
    "#                         print(NLL,KLD,Cross)\n",
    "            else:\n",
    "                NLL,KLD,Cross = loss_function(y_hat, confidences, data, label, mean1, std1, mean2, std2)\n",
    "                loss = NLL + (25)*KLD + 20*Cross\n",
    "#                 if i + 1>= len(train_dataloader)//1:\n",
    "#                     print(NLL,KLD,Cross)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "#             loss = loss / accumulation_steps\n",
    "#             loss.backward() \n",
    "#             if (i+1) % accumulation_steps == 0:\n",
    "#                 cvae_optimizer.step()\n",
    "#                 cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "            cvae_optimizer.zero_grad(set_to_none = True)\n",
    "            loss.backward()\n",
    "            cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "            del data, y_hat, confidences, mean1, std1, mean2, std2, NLL, KLD, Cross, loss\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.645Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    torch.save(cvae.state_dict(),'E:/Downloads/lyft-motion-prediction-autonomous-vehicles/cvae.pth')\n",
    "    plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode'] and cfg['train_params']['epochs'] > 1: \n",
    "    plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.648Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset[0].keys())\n",
    "# print(len(eval_dataset))\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    prefetch_factor = 2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory = True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")\n",
    "pred_path = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles/pred.csv\"\n",
    "eval_gt_path = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles/gt.csv\"\n",
    "cvae.load_state_dict(torch.load('E:/Downloads/lyft-motion-prediction-autonomous-vehicles/cvae.pth'))\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.649Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "losses_test = []\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "confs = []\n",
    "tr_it = iter(eval_dataloader)\n",
    "progress_bar = tqdm(range(len(eval_dataloader)//cfg['scale']),position=0)\n",
    "\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data,_ = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(eval_dataloader)\n",
    "        data,_ = next(tr_it)\n",
    "    y_hat, confidences,mean1,std1,mean2,std2 = cvae(data)\n",
    "#     if cfg[\"train_params\"][\"device\"] == 1:\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             NLL,KLD,Cross = loss_function(y_hat, confidences, data, mean1, std1, mean2, std2)\n",
    "#             loss = NLL + (25)*KLD + 20*Cross\n",
    "#     losses_test.append(loss.item())\n",
    "#     progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_test)}\")\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.detach().cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_off = []\n",
    "    for i in range(num_classes):\n",
    "        coords_off.append(transform_points(agents_coords[:,i,:,:], world_from_agents) - centroids[:, None, :2])\n",
    "#         coords_offset2 = transform_points(agents_coords[:,1,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "#         coords_offset3 = transform_points(agents_coords[:,2,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "    coords_offset = np.stack([coords_offseti for coords_offseti in coords_off],1)\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "    confs.append(confidences.detach().cpu().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.651Z"
    }
   },
   "outputs": [],
   "source": [
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "               confs=np.concatenate(confs)\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.652Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[10//cfg[\"model_params\"][\"future_step_size\"]-1], FDE[30//cfg[\"model_params\"][\"future_step_size\"]-1], FDE[50//cfg[\"model_params\"][\"future_step_size\"]-1], np.mean(FDE[:10//cfg[\"model_params\"][\"future_step_size\"]]), np.mean(FDE[:30//cfg[\"model_params\"][\"future_step_size\"]]), np.mean(FDE[:50//cfg[\"model_params\"][\"future_step_size\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-15T07:24:33.654Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_vis = False\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    predicted_positions1 = []\n",
    "    predicted_positions2 = []\n",
    "    predicted_positions3 = []\n",
    "    target_positions = []\n",
    "\n",
    "    if multi_vis == True:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "            print(confs)\n",
    "            out_net1 = out_net[0][0]\n",
    "            out_net2 = out_net[0][1]\n",
    "            out_net3 = out_net[0][2]\n",
    "            out_pos1 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos2 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos3 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions1.append(transform_points(out_pos1, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions2.append(transform_points(out_pos2, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions3.append(transform_points(out_pos3, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions1 = transform_points(np.concatenate(predicted_positions1), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions2 = transform_points(np.concatenate(predicted_positions2), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions3 = transform_points(np.concatenate(predicted_positions3), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, predicted_positions1, (34,222,79))\n",
    "        draw_trajectory(im_ego, predicted_positions2, (220,235,21))\n",
    "        draw_trajectory(im_ego, predicted_positions3, PREDICTED_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "    #         print(confs)\n",
    "            out_net = out_net[0][np.argmax(confs)]\n",
    "            out_pos = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stgm",
   "language": "python",
   "name": "stgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
