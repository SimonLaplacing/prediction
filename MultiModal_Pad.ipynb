{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mydataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'mode': {'load_mode': True}, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 256, 'num_layers': 2, 'bidirectional': True, 'history_step_size': 1, 'history_num_frames': 49, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_mode': 1, 'raster_size': [112, 112], 'pixel_size': [0.75, 0.75], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 2}, 'val_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 2}, 'train_params': {'device': 1, 'epochs': 1}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ===== INIT DATASET\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset)\n",
    "    print(train_dataset[0].keys())\n",
    "\n",
    "    train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"], \n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        prefetch_factor = 2,\n",
    "        pin_memory = True,\n",
    "        persistent_workers=True,\n",
    "        worker_init_fn=my_dataset_worker_init_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数\n",
    "\n",
    "num_classes = 3 # 类数\n",
    "modal_fc = latent_dim*(1+bidirectional) \n",
    "thre = 0.175 # yaw_threshold=10°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # 定义编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.encoder2 = nn.Linear(latent_dim*(1+bidirectional), encoder_fc)\n",
    "#         self.encoder2 = nn.Linear(latent_dim*(1+bidirectional)+modal_fc, encoder_fc)\n",
    "#         self.encoder_mean1 = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.encoder_mean2 = nn.Linear(encoder_fc, z_dimension)\n",
    "#         self.encoder_std1 = nn.Linear(latent_dim*(1+bidirectional), 32)\n",
    "        self.encoder_std2 = nn.Linear(encoder_fc, z_dimension)\n",
    "\n",
    "        # 定义解码器\n",
    "        self.decoder = nn.LSTM(z_dimension*2, latent_dim, num_layers=num_layers,\n",
    "                               bidirectional=bidirectional, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.decoder_fc1 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_fc2 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_fc3 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_confi = nn.Linear(64, num_classes)\n",
    "\n",
    "        # 道路特征提取器\n",
    "        # load pre-trained Conv2D model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        # change input channels number to match the rasterizer's output\n",
    "        num_history_channels = (\n",
    "            cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.resnet.conv1.out_channels,\n",
    "            kernel_size=self.resnet.conv1.kernel_size,\n",
    "            stride=self.resnet.conv1.stride,\n",
    "            padding=self.resnet.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # change output size to (X, Y) * number of future states\n",
    "        num_targets = z_dimension * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    def noise_reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward(self, data):\n",
    "        inputs1 = torch.FloatTensor(data[\"history_positions\"]).to(device)\n",
    "#         yaw = torch.FloatTensor(data[\"history_yaws\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1 = torch.unsqueeze(inputs1, 0)\n",
    "\n",
    "        h0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "\n",
    "        inputs2 = torch.FloatTensor(data[\"image\"]).to(device)\n",
    "        if inputs2.dim() == 3:\n",
    "            inputs2 = torch.unsqueeze(inputs2, 0)\n",
    "\n",
    "        out1, _ = self.encoder(inputs1, (h0, c0))\n",
    "        out1 = out1[:,-1,:]\n",
    "        out1 = torch.unsqueeze(out, 1)\n",
    "        out1 = out.expand(out.size()[0],decoder_length,out.size()[-1])\n",
    "        out1 = F.relu(self.encoder2(out1), inplace=True)\n",
    "\n",
    "#         mean1 = F.relu(self.encoder_mean1(out1), inplace=True)\n",
    "        mean2 = F.relu(self.encoder_mean2(out1), inplace=True)\n",
    "#         logstd1 = F.relu(self.encoder_std1(out1), inplace=True)\n",
    "        logstd2 = F.relu(self.encoder_std2(out1), inplace=True)\n",
    "        z1 = self.noise_reparameterize(mean2, logstd2)\n",
    "        z2 = self.resnet(inputs2).reshape(z1.size())\n",
    "        z = torch.cat([z1, z2], -1)\n",
    "        out2, _ = self.decoder(z)\n",
    "        out2 = F.relu(self.decoder_fc(out2), inplace=True)\n",
    "\n",
    "        out21 = F.relu(self.decoder_fc1(out2), inplace=True)\n",
    "        out21 = torch.unsqueeze(out21, 1)\n",
    "\n",
    "        out22 = F.relu(self.decoder_fc2(out2), inplace=True)\n",
    "        out22 = torch.unsqueeze(out22, 1)\n",
    "\n",
    "        out23 = F.relu(self.decoder_fc3(out2), inplace=True)\n",
    "        out23 = torch.unsqueeze(out23, 1)\n",
    "\n",
    "        confidences = F.softmax(self.decoder_confi(out2)[:, -1, :], dim=-1)\n",
    "        y_hat = torch.cat([out21, out22, out23], dim=1)\n",
    "\n",
    "        return y_hat, confidences, mean2, logstd2\n",
    "\n",
    "def label_maker(yaw):\n",
    "    yaw = yaw.squeeze()\n",
    "#     print(yaw.size())\n",
    "    ind = torch.argmax(torch.abs(yaw),dim=-1)\n",
    "#     print(ind.size())\n",
    "    yaw = yaw[torch.arange(len(yaw)),ind]\n",
    "    yaw = yaw.unsqueeze(dim=-1)\n",
    "    w = torch.cat([yaw,yaw,yaw],dim=-1)\n",
    "    one = torch.ones_like(yaw)\n",
    "    label1 = torch.tensor([0.,0.,1.]).to(device)\n",
    "    label1 = label1.expand(len(yaw),num_classes)\n",
    "    label2 = torch.tensor([0.,1.,0.]).to(device)\n",
    "    label2 = label2.expand(len(yaw),num_classes)\n",
    "    label3 = torch.tensor([1.,0.,0.]).to(device)\n",
    "    label3 = label3.expand(len(yaw),num_classes)\n",
    "\n",
    "    w = torch.where(yaw >= thre, label1, w)\n",
    "    yaw = torch.where(yaw >= thre, one, yaw)\n",
    "\n",
    "    w = torch.where(yaw <= -thre, label3, w)\n",
    "    yaw = torch.where(yaw <= -thre, one, yaw)\n",
    "\n",
    "    w = torch.where(yaw<thre, label2, w)\n",
    "#     w = torch.LongTensor(w).to(device)\n",
    "    return w\n",
    "    \n",
    "def loss_function(y_hat, confidences, data, mean, std):\n",
    "    y_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    yaw = data[\"target_yaws\"].to(device)\n",
    "    label = label_maker(yaw)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "#     MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "#     MSE = MSE * y_availabilities\n",
    "#     MSE = MSE.mean()\n",
    "    Cross = F.binary_cross_entropy_with_logits(confidences,label)\n",
    "    NLL = neg_multi_log_likelihood_batch(\n",
    "        y_true, y_hat, confidences, y_availabilities)\n",
    "    # 因为var是标准差的自然对数，先求自然对数然后平方转换成方差\n",
    "    var = torch.pow(torch.exp(std), 2)\n",
    "    KLD = -0.5 * torch.mean(1+torch.log(var)-torch.pow(mean, 2)-var)\n",
    "#     print('KLD: ',KLD,' NLL: ',NLL,' Cross: ', Cross)\n",
    "    return (NLL + (10**6)*KLD + 5000*Cross)/3\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "# vae.load_state_dict(torch.load('./VAE_z2.pth'))\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)//100),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "            y_hat, confidences, mean, std = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = loss_function(y_hat, confidences, data, mean, std)\n",
    "            else:\n",
    "                loss = loss_function(y_hat, confidences, data, mean, std)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward() \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                cvae_optimizer.step()\n",
    "                cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "    #         cvae_optimizer.zero_grad(set_to_none = True)\n",
    "    #         loss.backward()\n",
    "    #         cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    torch.save(cvae.state_dict(),'E:/Downloads/cvae.pth')\n",
    "    plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode'] and cfg['train_params']['epochs'] > 1: \n",
    "    plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n",
      "6978\n"
     ]
    }
   ],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset[0].keys())\n",
    "# print(len(eval_dataset))\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    prefetch_factor = 2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory = True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")\n",
    "pred_path = \"E:/Downloads/pred.csv\"\n",
    "eval_gt_path = \"E:/Downloads/gt.csv\"\n",
    "cvae.load_state_dict(torch.load('E:/Downloads/cvae.pth'))\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13515/13515 [1:34:13<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "confs = []\n",
    "tr_it = iter(eval_dataloader)\n",
    "progress_bar = tqdm(range(len(eval_dataloader)//100),position=0)\n",
    "\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(eval_dataloader)\n",
    "        data = next(tr_it)\n",
    "    y_hat, confidences,_,_ = cvae(data)\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.detach().cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_offset1 = transform_points(agents_coords[:,0,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "    coords_offset2 = transform_points(agents_coords[:,1,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "    coords_offset3 = transform_points(agents_coords[:,2,:,:], world_from_agents) - centroids[:, None, :2]\n",
    "    coords_offset = np.stack([coords_offset1,coords_offset2,coords_offset3],1)\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "    confs.append(confidences.detach().cpu().numpy().copy())\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "               confs=np.concatenate(confs)\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_multi_log_likelihood 106.05650365334421\n",
      "time_displace [0.10831735 0.15208156 0.19169208 0.22418313 0.2607895  0.29848\n",
      " 0.33465559 0.36275734 0.3834866  0.40558187 0.42613386 0.45048882\n",
      " 0.47952913 0.50395157 0.53160466 0.55706076 0.58817849 0.6138989\n",
      " 0.64676583 0.67814166 0.70827302 0.73636999 0.76257243 0.78527188\n",
      " 0.81503998 0.84857733 0.87656759 0.90199651 0.92571771 0.94999342\n",
      " 0.97633007 1.00490055 1.024024   1.05127928 1.08678492 1.11549866\n",
      " 1.14605151 1.17220307 1.19937152 1.22394343 1.24652899 1.26841723\n",
      " 1.293328   1.31517316 1.33373825 1.35161387 1.36863196 1.39104932\n",
      " 1.41369712 1.50814765]\n",
      "FDE1s: 0.40558187224781234, FDE3s: 0.9499934233089199, FDE5s: 1.508147647081944, ADE1s: 0.27220250262678236, ADE3s: 0.5502719523445264, ADE5s: 0.8199774229332847 \n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[9], FDE[29], FDE[49], np.mean(FDE[:10]), np.mean(FDE[:30]), np.mean(FDE[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'11576254980902731486'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c4ff11885268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m# retrieve target positions from the GT and store as absolute coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mtrack_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_agent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"track_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_agent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"timestamp\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mtarget_positions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_agent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"centroid\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '11576254980902731486'"
     ]
    }
   ],
   "source": [
    "multi_vis = False\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    predicted_positions1 = []\n",
    "    predicted_positions2 = []\n",
    "    predicted_positions3 = []\n",
    "    target_positions = []\n",
    "\n",
    "    if multi_vis == True:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "\n",
    "            out_net1 = out_net[0][0]\n",
    "            out_net2 = out_net[0][1]\n",
    "            out_net3 = out_net[0][2]\n",
    "            out_pos1 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos2 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            out_pos3 = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions1.append(transform_points(out_pos1, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions2.append(transform_points(out_pos2, data_agent[\"world_from_agent\"]))\n",
    "            predicted_positions3.append(transform_points(out_pos3, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions1 = transform_points(np.concatenate(predicted_positions1), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions2 = transform_points(np.concatenate(predicted_positions2), data_ego[\"raster_from_world\"])\n",
    "        predicted_positions3 = transform_points(np.concatenate(predicted_positions3), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, predicted_positions1, (34,222,79))\n",
    "        draw_trajectory(im_ego, predicted_positions2, (220,235,21))\n",
    "        draw_trajectory(im_ego, predicted_positions3, PREDICTED_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        for v_index in agent_indices:\n",
    "            data_agent = eval_dataset[v_index]\n",
    "            out_net,confs,_,_ = cvae(data_agent)\n",
    "            confs = confs.detach().cpu().numpy()\n",
    "    #         print(confs)\n",
    "            out_net = out_net[0][np.argmax(confs)]\n",
    "            out_pos = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "            # store absolute world coordinates\n",
    "            predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "            # retrieve target positions from the GT and store as absolute coordinates\n",
    "            track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "            target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "        # convert coordinates to AV point-of-view so we can draw them\n",
    "        predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "        target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "        draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "\n",
    "\n",
    "        plt.imshow(im_ego)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python {lgsvl}",
   "language": "python",
   "name": "lgsvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
