{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mydataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 128, 'num_layers': 2, 'bidirectional': False, 'history_step_size': 1, 'history_num_frames': 49, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_size': [224, 224], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 2}, 'val_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 2}, 'train_params': {'device': 1, 'epochs': 1}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "# ===== INIT DATASET\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "print(len(train_dataset))\n",
    "print(train_dataset)\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = 1)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=train_cfg[\"shuffle\"], \n",
    "    batch_size=train_cfg[\"batch_size\"],\n",
    "    num_workers=train_cfg[\"num_workers\"],\n",
    "    pin_memory = True,\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "num_classes = 3 # 类数\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 16\n",
    "accumulation_steps = 3 # 梯度累积步数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        # 定义编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.encoder2 = nn.Linear(latent_dim*(1+bidirectional),16)\n",
    "#         self.encoder_mean1 = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.encoder_mean2 = nn.Linear(16*2, z_dimension)\n",
    "#         self.encoder_std1 = nn.Linear(latent_dim*(1+bidirectional), 32)\n",
    "        self.encoder_std2 = nn.Linear(16*2, z_dimension)\n",
    "    \n",
    "        # 定义解码器\n",
    "        self.decoder = nn.LSTM(z_dimension, latent_dim, num_layers=num_layers,\n",
    "                               bidirectional=bidirectional, batch_first=True)\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim*(1+bidirectional), 32)\n",
    "        self.decoder_fc2 = nn.Linear(32, 16)\n",
    "        self.decoder_fc3 = nn.Linear(16, num_decoder_tokens)\n",
    "        \n",
    "        # 道路特征提取器\n",
    "        # load pre-trained Conv2D model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        # change input channels number to match the rasterizer's output\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.resnet.conv1.out_channels,\n",
    "            kernel_size=self.resnet.conv1.kernel_size,\n",
    "            stride=self.resnet.conv1.stride,\n",
    "            padding=self.resnet.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # change output size to (X, Y) * number of future states\n",
    "        num_targets = 16 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    def noise_reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward(self, data):\n",
    "        inputs1 = torch.FloatTensor(data[\"history_positions\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1.resize_(1, inputs1.size()[0], inputs1.size()[1])\n",
    "#         print(inputs.size())\n",
    "        h0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        inputs2 = torch.FloatTensor(data[\"image\"]).to(device)\n",
    "        if inputs2.dim() == 3:\n",
    "            inputs2.resize_(1, inputs2.size()[0], inputs2.size()[1], inputs2.size()[2])\n",
    "        out11, _ = self.encoder(inputs1, (h0, c0))\n",
    "        out11 = F.relu(self.encoder2(out11), inplace=True)\n",
    "        out12 = self.resnet(inputs2).reshape(out11.size())\n",
    "        out1 = torch.cat([out11,out12],2)\n",
    "#         print(out1.size())\n",
    "#         mean1 = F.relu(self.encoder_mean1(out1), inplace=True)\n",
    "        mean2 = F.relu(self.encoder_mean2(out1), inplace=True)\n",
    "#         logstd1 = F.relu(self.encoder_std1(out1), inplace=True)\n",
    "        logstd2 = F.relu(self.encoder_std2(out1), inplace=True)\n",
    "        z = self.noise_reparameterize(mean2, logstd2)\n",
    "        out2, _ = self.decoder(z)\n",
    "        out2 = F.relu(self.decoder_fc1(out2), inplace=True)\n",
    "        out2 = F.relu(self.decoder_fc2(out2), inplace=True)\n",
    "        y_hat = self.decoder_fc3(out2)\n",
    "        return y_hat, mean2, logstd2\n",
    "\n",
    "\n",
    "def loss_function(y_hat, data, mean, std):\n",
    "    y_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "    MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    MSE = MSE * y_availabilities\n",
    "    MSE = MSE.mean()\n",
    "    # 因为var是标准差的自然对数，先求自然对数然后平方转换成方差\n",
    "    var = torch.pow(torch.exp(std), 2)\n",
    "    KLD = -0.5 * torch.mean(1+torch.log(var)-torch.pow(mean, 2)-var)\n",
    "    return MSE+KLD\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "# vae.load_state_dict(torch.load('./VAE_z2.pth'))\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(),lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19814328849315643 loss(avg): 11.485886489416721: 100%|███████████████████| 6978/6978 [2:27:25<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# ==== TRAIN LOOP\n",
    "losses_avg = []\n",
    "for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "    tr_it = iter(train_dataloader)\n",
    "    progress_bar = tqdm(range(len(train_dataloader)),position=0)\n",
    "    losses_train = []\n",
    "    cvae_optimizer.zero_grad(set_to_none = True)\n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "        cvae.train() # 设置为训练模式\n",
    "        torch.set_grad_enabled(True)\n",
    "        y_hat, mean, std = cvae(data)  # 输入\n",
    "        if cfg[\"train_params\"][\"device\"] == 1:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = loss_function(y_hat, data, mean, std)\n",
    "        else:\n",
    "            loss = loss_function(y_hat, data, mean, std)\n",
    "\n",
    "        # Backward pass\n",
    "        # 梯度累积模式\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward() \n",
    "        if (i+1) % accumulation_steps == 0:\n",
    "            cvae_optimizer.step()\n",
    "            cvae_optimizer.zero_grad(set_to_none = True)\n",
    "            \n",
    "        # 无梯度累积模式\n",
    "#         cvae_optimizer.zero_grad(set_to_none = True)\n",
    "#         loss.backward()\n",
    "#         cvae_optimizer.step()\n",
    "        losses_train.append(loss.item())\n",
    "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "    losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111634\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|    100     |   24838    |  1893736   |     316008    |       0.69      |        248.38        |        76.24         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0].keys())\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = 1)\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    persistent_workers=True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6978/6978 [2:21:57<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "progress_bar = tqdm(eval_dataloader,position=0)\n",
    "\n",
    "for data in progress_bar:\n",
    "    y_hat, mean, std = cvae(data)\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_offset = transform_points(agents_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "    \n",
    "pred_path = f\"{gettempdir()}/pred.csv\"\n",
    "eval_gt_path = f\"{gettempdir()}/gt.csv\"\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_multi_log_likelihood 115.85915414857315\n",
      "time_displace [0.3987453  0.33772676 0.36434189 0.40991574 0.4513414  0.48041841\n",
      " 0.49506552 0.51146123 0.53511304 0.55873684 0.57812765 0.59202755\n",
      " 0.60763718 0.62540379 0.64543137 0.66776716 0.68957458 0.71067697\n",
      " 0.73157773 0.75196526 0.77102876 0.789499   0.80894398 0.83048667\n",
      " 0.85154206 0.87206777 0.89207013 0.91357206 0.93463478 0.95514398\n",
      " 0.97501236 0.9944503  1.01427094 1.03265439 1.05011385 1.06774697\n",
      " 1.08567958 1.10630339 1.13054285 1.15791878 1.18900733 1.2235722\n",
      " 1.26516302 1.31350887 1.36967223 1.43230178 1.50118505 1.58966021\n",
      " 1.68695431 1.81993951]\n",
      "FDE1s: 0.5587368398879811, FDE3s: 0.9551439839078752, FDE5s: 1.8199395069875237, ADE1s: 0.45428661324989417, ADE3s: 0.6587348196915888, ADE5s: 0.8953540498201533 \n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[9], FDE[29], FDE[49], np.mean(FDE[:10]), np.mean(FDE[:30]), np.mean(FDE[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 103, 7, 7], but got 3-dimensional input of size [103, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5a8e301c2066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0magent_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdata_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mout_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mout_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_net\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# store absolute world coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c291629162bc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mout11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mout11\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mout12\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout11\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#         print(out1.size())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# See note [TorchScript super()]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 396\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 103, 7, 7], but got 3-dimensional input of size [103, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    target_positions = []\n",
    "\n",
    "    for v_index in agent_indices:\n",
    "        data_agent = eval_dataset[v_index]\n",
    "        out_net = cvae(data_agent)\n",
    "        out_pos = out_net[0].reshape(-1, 2).detach().cpu().numpy()\n",
    "        # store absolute world coordinates\n",
    "        predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "        # retrieve target positions from the GT and store as absolute coordinates\n",
    "        track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "        target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "    # convert coordinates to AV point-of-view so we can draw them\n",
    "    predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "    target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.imshow(im_ego)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python {lgsvl}",
   "language": "python",
   "name": "lgsvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
