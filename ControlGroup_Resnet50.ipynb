{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\l5kit\\dataset\\select_agents.py:32: UserWarning: Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.However, writing the mask with this config may be inconsistent.\n",
      "  \"Windows detected. BLOSC_NOLOCK has not been set as it causes memory leaks on Windows.\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50,resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mydataset import MyTrainDataset, my_dataset_worker_init_func\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, write_gt_csv, read_gt_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, average_displacement_error_mean, final_displacement_error_mean\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'mode': {'load_mode': False}, 'model_params': {'model_architecture': 'CVAE', 'latent_dim': 256, 'num_layers': 2, 'bidirectional': True, 'history_step_size': 1, 'history_num_frames': 49, 'future_step_size': 1, 'future_num_frames': 50, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_mode': 1, 'raster_size': [186, 186], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 16, 'shuffle': True, 'num_workers': 2}, 'val_data_loader': {'key': 'scenes/validate.zarr', 'batch_size': 16, 'shuffle': False, 'num_workers': 2}, 'train_params': {'device': 1, 'epochs': 1}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"E:/Downloads/lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22496709\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ===== INIT DATASET\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "    print(len(train_dataset))\n",
    "    print(train_dataset)\n",
    "    print(train_dataset[0].keys())\n",
    "\n",
    "    train_dataset = MyTrainDataset(cfg, dm, len(train_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=train_cfg[\"shuffle\"], \n",
    "        batch_size=train_cfg[\"batch_size\"],\n",
    "        num_workers=train_cfg[\"num_workers\"],\n",
    "        prefetch_factor = 2,\n",
    "        pin_memory = True,\n",
    "        persistent_workers=True,\n",
    "        worker_init_fn=my_dataset_worker_init_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "num_classes = 3 # 类数\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.model = resnet50(pretrained=True)\n",
    "\n",
    "        # change input channels number to match the rasterizer's output\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "        self.model.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.model.conv1.out_channels,\n",
    "            kernel_size=self.model.conv1.kernel_size,\n",
    "            stride=self.model.conv1.stride,\n",
    "            padding=self.model.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # change output size to (X, Y) * number of future states\n",
    "        num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        self.model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    def forward(self, data):\n",
    "        inputs1 = torch.FloatTensor(data[\"image\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1 = torch.unsqueeze(inputs1,0)\n",
    "\n",
    "        y_hat = self.model(inputs1).reshape(data[\"target_positions\"].shape)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "def loss_function(y_hat, data):\n",
    "    y_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "    MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "    MSE = MSE * y_availabilities\n",
    "    MSE = MSE.mean()\n",
    "    return MSE\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.10305219143629074 loss(avg): 1.030415100343181: 100%|██████████████████| 14060/14060 [3:17:46<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)//100),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "            y_hat = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = loss_function(y_hat, data)\n",
    "            else:\n",
    "                loss = loss_function(y_hat, data)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward() \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                cvae_optimizer.step()\n",
    "                cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "    #         cvae_optimizer.zero_grad(set_to_none = True)\n",
    "    #         loss.backward()\n",
    "    #         cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    torch.save(cvae.state_dict(),'E:/Downloads/cvae.pth')\n",
    "    plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cfg['mode']['load_mode'] and cfg['train_params']['epochs'] > 1: \n",
    "    plt.plot(np.arange(len(losses_avg)), losses_avg, label=\"train loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21624612\n",
      "dict_keys(['frame_index', 'image', 'target_positions', 'target_yaws', 'target_velocities', 'target_availabilities', 'history_positions', 'history_yaws', 'history_velocities', 'history_availabilities', 'world_to_image', 'raster_from_agent', 'raster_from_world', 'agent_from_world', 'world_from_agent', 'centroid', 'yaw', 'extent', 'history_extents', 'future_extents', 'curr_speed', 'scene_index', 'host_id', 'timestamp', 'track_id'])\n",
      "1351539\n"
     ]
    }
   ],
   "source": [
    "eval_cfg = cfg[\"val_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "print(len(eval_dataset))\n",
    "print(eval_dataset[0].keys())\n",
    "\n",
    "eval_dataset = MyTrainDataset(cfg, dm, len(eval_dataset),raster_mode = cfg[\"raster_params\"][\"raster_mode\"])\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=eval_cfg[\"shuffle\"], \n",
    "    batch_size=eval_cfg[\"batch_size\"],\n",
    "    num_workers=eval_cfg[\"num_workers\"],\n",
    "    prefetch_factor = 2,\n",
    "    persistent_workers=True,\n",
    "    pin_memory = True,\n",
    "    worker_init_fn=my_dataset_worker_init_func\n",
    ")\n",
    "\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13515/13515 [2:54:38<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==== EVAL LOOP\n",
    "cvae.load_state_dict(torch.load('E:/Downloads/cvae.pth'))\n",
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "gt_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "availability = []\n",
    "confs = []\n",
    "tr_it = iter(eval_dataloader)\n",
    "progress_bar = tqdm(range(len(eval_dataloader)//100),position=0)\n",
    "\n",
    "for i in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(eval_dataloader)\n",
    "        data = next(tr_it)\n",
    "    y_hat = cvae(data)\n",
    "#     print(data)\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = y_hat.detach().cpu().numpy()\n",
    "    gt_coords = data['target_positions'].numpy()\n",
    "    world_from_agents = data['world_from_agent'].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_offset = transform_points(agents_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    gt_offset = transform_points(gt_coords, world_from_agents) - centroids[:, None, :2]\n",
    "    \n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    gt_coords_offsets_pd.append(np.stack(gt_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n",
    "    availability.append(data[\"target_availabilities\"].numpy().copy())\n",
    "\n",
    "    \n",
    "pred_path = \"E:/Downloads/pred.csv\"\n",
    "eval_gt_path = \"E:/Downloads/gt.csv\"\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )\n",
    "\n",
    "write_gt_csv(eval_gt_path,timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(gt_coords_offsets_pd),avails=np.concatenate(availability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_multi_log_likelihood 88.12073837934946\n",
      "time_displace [0.06745394 0.12784749 0.17139182 0.21276629 0.25479284 0.29530414\n",
      " 0.32772036 0.35828544 0.3919001  0.42419402 0.44056774 0.46702333\n",
      " 0.47355578 0.50192835 0.53421933 0.5610397  0.57826826 0.60388651\n",
      " 0.63084565 0.65659568 0.67418061 0.69412069 0.71927255 0.74929798\n",
      " 0.7747119  0.80485564 0.82486634 0.84923431 0.8789555  0.9038547\n",
      " 0.92480573 0.95370952 0.9806096  1.00718885 1.03280342 1.06339804\n",
      " 1.08623064 1.12021344 1.1461287  1.17953624 1.2022255  1.22704425\n",
      " 1.26490194 1.28897845 1.3167712  1.34926612 1.38015361 1.4175873\n",
      " 1.44692699 1.4762596 ]\n",
      "FDE1s: 0.42419401733054235, FDE3s: 0.9038547030261782, FDE5s: 1.476259604647944, ADE1s: 0.2631656440911759, ADE3s: 0.5317645661406806, ADE5s: 0.7963535226412438 \n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [\n",
    "                              neg_multi_log_likelihood, time_displace])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n",
    "    if metric_name==\"time_displace\":\n",
    "        FDE = metric_mean\n",
    "print('FDE1s: {}, FDE3s: {}, FDE5s: {}, ADE1s: {}, ADE3s: {}, ADE5s: {} '.format(\n",
    "    FDE[9], FDE[29], FDE[49], np.mean(FDE[:10]), np.mean(FDE[:30]), np.mean(FDE[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 298. MiB for an array with shape (312617887,) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b40684305aa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0meval_ego_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEgoDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_zarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0meval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgentDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_zarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrasterizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mframe_number\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_zarr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# start from last frame of scene_0 and increase by 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\lgsvl\\lib\\site-packages\\l5kit\\dataset\\agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, cfg, zarr_dataset, rasterizer, perturbation, agents_mask, min_frame_history, min_frame_future)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0magents_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if not provided try to load it from the zarr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0magents_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_agents_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mpast_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmin_frame_history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mfuture_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmin_frame_future\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0magents_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_mask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfuture_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 298. MiB for an array with shape (312617887,) and data type bool"
     ]
    }
   ],
   "source": [
    "cvae.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_zarr, rasterizer)\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number) \n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    \n",
    "    predicted_positions = []\n",
    "    predicted_positions1 = []\n",
    "    predicted_positions2 = []\n",
    "    predicted_positions3 = []\n",
    "    target_positions = []\n",
    "\n",
    "    for v_index in agent_indices:\n",
    "        data_agent = eval_dataset[v_index]\n",
    "        out_net = cvae(data_agent)\n",
    "        out_net = out_net[0]\n",
    "        out_pos = out_net.reshape(-1, 2).detach().cpu().numpy()\n",
    "        # store absolute world coordinates\n",
    "        predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "        # retrieve target positions from the GT and store as absolute coordinates\n",
    "        track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "        target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "    # convert coordinates to AV point-of-view so we can draw them\n",
    "    predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "    target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "\n",
    "\n",
    "    plt.imshow(im_ego)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python {lgsvl}",
   "language": "python",
   "name": "lgsvl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
